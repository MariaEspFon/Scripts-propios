{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaEspFon/Scripts-propios/blob/main/TensorFlow/CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKNAOn5Ia7pG"
      },
      "source": [
        "\n",
        "# MODELO 1: red neuronal convolucional (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhxVlFL7j4N6"
      },
      "source": [
        "## 1. Inicialización de Keras y TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcFrOCQOYU1R"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "#print(\"Keras version: \", tf.keras.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtndO_D__Z2T"
      },
      "source": [
        "## 2. Carga de datos EDA desde Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niPo3Nfh-bVz"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/MariaEspFon/Scripts-propios/main/MATLAB/EDA_data.csv'\n",
        "#url = 'https://raw.githubusercontent.com/MariaEspFon/Scripts-propios/main/MATLAB/EDA_data%20(II).csv' # generado con enventanado de 20s en lugar de 30s\n",
        "column_names = ['Median', 'Standard Dev', 'Max Value', 'Min Value', 'Median 1st diff', 'Standard Dev 1st diff', 'Number of SCR', 'Total Area', 'Power', 'State' ]\n",
        "raw_dataset = pd.read_csv(url, names=column_names, sep=',', skipinitialspace=True)\n",
        "\n",
        "size = raw_dataset.shape\n",
        "print(f'Formato del dataset: {size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEvgfenf-pEf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "raw_dataset.tail() # muestra las últimas 5 filas por defecto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Cqjf_xnwpw"
      },
      "outputs": [],
      "source": [
        "raw_dataset.head() # muestra las primeras 5 filas por defecto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9-FGkx3glzZ"
      },
      "source": [
        "## 3. Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBXjhOwa_eFh"
      },
      "source": [
        "### 3.1. Extracción de los conjuntos de entrenamiento, prueba y validación\n",
        "\n",
        "*   Datos de **entrenamiento**: para el aprendizaje de parámetros.\n",
        "*   Datos de **prueba**: para hacer test de predicciones.\n",
        "*   Datos de **validación**: para afinar hiperparámetros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yrXBybR_V7j"
      },
      "outputs": [],
      "source": [
        "train_dataset = raw_dataset.sample(frac=0.8,random_state=0)\n",
        "# random_state=0 marca la semilla aleatoria para escoger muestras\n",
        "test_dataset = raw_dataset.drop(train_dataset.index)\n",
        "# drop coge el resto de datos desde la posición final de train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtq9cEZV_V-m"
      },
      "outputs": [],
      "source": [
        "train_labels = train_dataset.pop('State')\n",
        "test_labels = test_dataset.pop('State')\n",
        "del train_dataset['Number of SCR']\n",
        "del test_dataset['Number of SCR']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FekCNTKP_WBI"
      },
      "outputs": [],
      "source": [
        "train_stats = train_dataset.describe()\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syU3bAgjhaZb"
      },
      "source": [
        "### 3.2. Normalización y estandarización de todos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6-D516VCSUe"
      },
      "outputs": [],
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)\n",
        "train_size = normed_train_data.shape\n",
        "test_size = normed_test_data.shape\n",
        "print(f'Formato del dataset de training: {train_size}')\n",
        "print(f'Formato del dataset de test: {test_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5PUc7EgFtCn"
      },
      "outputs": [],
      "source": [
        "normed_train_stats = normed_train_data.describe()\n",
        "normed_train_stats = normed_train_stats.transpose()\n",
        "normed_train_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmNpCfOqCwqv"
      },
      "source": [
        "## 4. Creación del modelo CNN\n",
        "\n",
        "El primer modelo cuenta con 2 tipos de capas:\n",
        "*     Capas convolucionales con un número variable de filtros de (2x1) o (3x1).\n",
        "*     Capas densas en la salida para integrar toda la información convolucional y hacer la clasificación binaria (para la cual sólo es necesaria una neurona y una función sigmoid de activación).\n",
        "\n",
        "Además, se incluye una capa accesoria para preparar los datos antes de la capa de salida (capa Flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aydxA-K9inqG"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential, layers\n",
        "\n",
        "def build_model(size):\n",
        "  model = Sequential()\n",
        "  model.add(layers.Conv1D(27, 2, strides=1, padding='same', activation='relu', input_shape=(size[1],1)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Conv1D(22, 3, padding='same', activation='relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Conv1D(18, 4, padding='same', activation='relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(22, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "model = build_model(train_size)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorporamos el callback de TensorBoard para poder guardar los parámetros, descripción y resultados de cada prueba con el modelo"
      ],
      "metadata": {
        "id": "tODAJp8wfkt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import callbacks\n",
        "import os\n",
        "\n",
        "# Crear el directorio donde se guardarán los logs de TensorBoard\n",
        "# log_dir = os.path.join(\"logs\", \"fit\", \"experiment_name\")  # Puedes personalizar el nombre\n",
        "# tensorboard_callback = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
      ],
      "metadata": {
        "id": "zJq-Qymzfj_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWp8Vcl0CShg"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdfftlXWEG-e"
      },
      "source": [
        "## 5. Entrenamiento del modelo\n",
        "\n",
        "Reservamos el 20% de los datos de entrenamiento para la validación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOaq8eGtCSkd"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 600\n",
        "\n",
        "history = model.fit(normed_train_data, train_labels, batch_size=511,\n",
        "                    epochs=EPOCHS, validation_split = 0.2, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3FgAK4CCSnX"
      },
      "outputs": [],
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHxBjzT8Ep82"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Binary Crossentropy')\n",
        "  plt.plot(hist['epoch'], hist['accuracy'],'r--',\n",
        "           label='Training Accuracy')\n",
        "  plt.plot(hist['epoch'], hist['val_accuracy'],'b',\n",
        "           label = 'Validation Accuracy')\n",
        "  plt.ylim([0,1])\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnwnYCxcEqCw"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Validación del modelo con los datos de test\n"
      ],
      "metadata": {
        "id": "O9M9YKiC-295"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS6oqzXxT3AJ"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(normed_test_data,  test_labels, batch_size=(test_size[0]))\n",
        "print(f'Test accuracy: {100*test_acc:.2f}%')\n",
        "print(f'Test loss: {test_loss:.3f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}